import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from wxauto import *
import time

from model import MyGLM4

system_prompt = "ä½ æ˜¯Dobyï¼Œåˆšåˆšä»ä¸Šæµ·äº¤é€šå¤§å­¦æ¯•ä¸šï¼Œå³å°†åˆ°å¤æ—¦å¤§å­¦è¯»ç ”ç©¶ç”Ÿï¼Œä»äº‹å¯ä¿¡äººå·¥æ™ºèƒ½çš„ç ”ç©¶ã€‚ä¸‹é¢çš„å¯¹è¯æ¥è‡ªäºä½ çš„å°ä¼™ä¼´ï¼Œä½ è¦å¥½å¥½åœ°å›å¤ä»–ï¼Œæœ€å¥½ä¿æŒç²¾ç®€ã€ä¸€å¥è¯å›å¤ã€‚"

listen_list = [
    'æ–‡ä»¶ä¼ è¾“åŠ©æ‰‹',
    'å¹²æ‹‰ ç™½ç»™ åˆ·æŠ–éŸ³',
]

device = "cuda" 

print("system_prompt:", system_prompt)
model = MyGLM4(
    model_path = "./glm-4-9b-chat-int4",
    max_new_tokens = 512,
    do_sample = True,
    top_k = 5,
    system_prompt = system_prompt,
    device = device,
    multi_user_list = listen_list
)



wx = WeChat()


# å¾ªç¯æ·»åŠ ç›‘å¬å¯¹è±¡
for i in listen_list:
    print(f"æ·»åŠ ç›‘å¬å¯¹è±¡{i}")
    wx.AddListenChat(who=i, savepic=False)

# æŒç»­ç›‘å¬æ¶ˆæ¯ï¼Œå¹¶ä¸”æ”¶åˆ°æ¶ˆæ¯åå›å¤â€œæ”¶åˆ°â€
wait = 1  # è®¾ç½®1ç§’æŸ¥çœ‹ä¸€æ¬¡æ˜¯å¦æœ‰æ–°æ¶ˆæ¯0

# 1åˆ†é’Ÿæ²¡æœ‰æ–°çš„å¯¹è¯åˆ™é‡Šæ”¾å¯¹è¯å†…å­˜
# ä¸ºæ¯ä¸ªç”¨æˆ·ç»´æŠ¤ä¸€ä¸ªè®¡æ•°å™¨
chat_release_time = 60
release_count_list = {}
for user_id in listen_list:
    release_count_list[user_id] = 0

def memory_check():
    for user_id in listen_list:
        if release_count_list[user_id] >= chat_release_time:
            print(f"é‡Šæ”¾{user_id}çš„å¯¹è¯å†…å­˜")
            model.release_chat_memory(user_id)
            release_count_list[user_id] = 0


print("å¼€å§‹ç›‘å¬")
while True:
    msgs = wx.GetListenMessage()

    for chat in msgs:
        who = chat.who              # è·å–èŠå¤©çª—å£åï¼ˆäººæˆ–ç¾¤åï¼‰
        one_msgs = msgs.get(chat)   # è·å–æ¶ˆæ¯å†…å®¹
        # å›å¤æ”¶åˆ°
        for msg in one_msgs:
            # print("æ”¶åˆ°æ¶ˆæ¯")

            msgtype = msg.type       # è·å–æ¶ˆæ¯ç±»å‹
            content = msg.content    # è·å–æ¶ˆæ¯å†…å®¹ï¼Œå­—ç¬¦ä¸²ç±»å‹çš„æ¶ˆæ¯å†…å®¹
            print(f'ã€{who}ã€‘ï¼š{content}')

            # æ¸…é™¤whoçš„è®¡æ•°å™¨
            release_count_list[who] = 0

            if who ==  "å¹²æ‹‰ ç™½ç»™ åˆ·æŠ–éŸ³":
                # print("æ”¶åˆ°"+who+"çš„æ¶ˆæ¯:"+content)
                if "æœ‰æ— " in content and msgtype == 'friend':
                    chat.SendMsg("ğŸ¤£åŒ…æœ‰çš„")    
                if "ä½•æ—¶" in content and msgtype == 'friend':
                    chat.SendMsg("ğŸ¤£ç­‰ä¸äº†äº†ï¼Œå°±ç°åœ¨")
                if "doby" not in content and "Doby" not in content:
                    continue
                else:
                    if msgtype == 'friend' or (msgtype == 'self' and "ğŸ¤£" not in content):
                        query = content
                        response = model.get_response(query, who)
                        chat.SendMsg("ğŸ¤£"+response[1:])
            elif who == "Framehehe":
                if msgtype == 'friend':
                    query = content
                    response = model.get_response(query, who)
                    chat.SendMsg("ğŸ¤£"+response[1:])
            elif who == "æ–‡ä»¶ä¼ è¾“åŠ©æ‰‹":
                print("æ”¶åˆ°"+who+"çš„æ¶ˆæ¯:"+content)
                if "ğŸ¤£" in content:
                    continue
                query = content
                response = model.get_response(query, who)
                chat.SendMsg("ğŸ¤£"+response[1:])
            else:
                if msgtype == 'friend':
                    query = content
                    response = model.get_response(query, who)
                    chat.SendMsg(response[1:])

           

    time.sleep(wait)
    # ç»´æŠ¤è®¡æ•°å™¨
    for user_id in listen_list:
        release_count_list[user_id] += wait
    memory_check()
    # æ˜¾å­˜æ£€æŸ¥ï¼Œè·å–ä»¥GBä¸ºå•ä½çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    gpu_memory = torch.cuda.memory_allocated() / 1024 ** 3
    # release memory if gpu memory is over 14GB
    if gpu_memory > 14:
        print(f"æ˜¾å­˜ä½¿ç”¨è¶…è¿‡14GBï¼Œé‡Šæ”¾æ‰€æœ‰å¯¹è¯å†…å­˜")
        for user_id in listen_list:
            model.release_chat_memory(user_id)
            release_count_list[user_id] = 0